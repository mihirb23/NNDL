{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFVxWZGJxprU"
   },
   "source": [
    "# Question B2 (10 marks)\n",
    "In Question B1, we used the Category Embedding model. This creates a feedforward neural network in which the categorical features get learnable embeddings. In this question, we will make use of a library called Pytorch-WideDeep. This library makes it easy to work with multimodal deep-learning problems combining images, text, and tables. We will just be utilizing the deeptabular component of this library through the TabMlp network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EycCozG06Duu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-widedeep\n",
      "  Using cached pytorch_widedeep-1.6.3-py3-none-any.whl (21.9 MB)\n",
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-3.2.0-py3-none-any.whl (255 kB)\n",
      "Collecting einops\n",
      "  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 KB\u001b[0m \u001b[31m748.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting gensim\n",
      "  Downloading gensim-4.3.3-cp310-cp310-macosx_11_0_arm64.whl (24.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting pyarrow\n",
      "  Downloading pyarrow-17.0.0-cp310-cp310-macosx_11_0_arm64.whl (27.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.2/27.2 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: pandas>=1.3.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pytorch-widedeep) (2.2.3)\n",
      "Collecting wrapt\n",
      "  Downloading wrapt-1.16.0-cp310-cp310-macosx_11_0_arm64.whl (38 kB)\n",
      "Collecting torchmetrics\n",
      "  Downloading torchmetrics-1.4.3-py3-none-any.whl (869 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m869.5/869.5 KB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting fastparquet>=0.8.1\n",
      "  Downloading fastparquet-2024.5.0-cp310-cp310-macosx_11_0_arm64.whl (683 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m683.8/683.8 KB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pytorch-widedeep) (4.66.5)\n",
      "Collecting imutils\n",
      "  Using cached imutils-0.5.4.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting scipy<=1.12.0,>=1.7.3\n",
      "  Downloading scipy-1.12.0-cp310-cp310-macosx_12_0_arm64.whl (31.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: transformers in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pytorch-widedeep) (4.45.2)\n",
      "Collecting torch>=2.0.0\n",
      "  Downloading torch-2.4.1-cp310-none-macosx_11_0_arm64.whl (62.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchvision>=0.15.0\n",
      "  Downloading torchvision-0.19.1-cp310-cp310-macosx_11_0_arm64.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hCollecting spacy\n",
      "  Downloading spacy-3.8.2-cp310-cp310-macosx_11_0_arm64.whl (6.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting opencv-contrib-python\n",
      "  Downloading opencv_contrib_python-4.10.0.84-cp37-abi3-macosx_11_0_arm64.whl (63.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=1.0.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pytorch-widedeep) (1.5.2)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-macosx_11_0_arm64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.21.6 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pytorch-widedeep) (1.26.4)\n",
      "Collecting cramjam>=2.3\n",
      "  Downloading cramjam-2.8.4-cp310-cp310-macosx_11_0_arm64.whl (1.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: packaging in /Users/mihirbhupathiraju/Library/Python/3.10/lib/python/site-packages (from fastparquet>=0.8.1->pytorch-widedeep) (23.1)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from fastparquet>=0.8.1->pytorch-widedeep) (2024.9.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/mihirbhupathiraju/Library/Python/3.10/lib/python/site-packages (from pandas>=1.3.5->pytorch-widedeep) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas>=1.3.5->pytorch-widedeep) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas>=1.3.5->pytorch-widedeep) (2024.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn>=1.0.2->pytorch-widedeep) (3.5.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn>=1.0.2->pytorch-widedeep) (1.4.2)\n",
      "Collecting sympy\n",
      "  Downloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch>=2.0.0->pytorch-widedeep) (3.4)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch>=2.0.0->pytorch-widedeep) (3.16.1)\n",
      "Collecting typing-extensions>=4.8.0\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: jinja2 in /Users/mihirbhupathiraju/Library/Python/3.10/lib/python/site-packages (from torch>=2.0.0->pytorch-widedeep) (3.1.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torchvision>=0.15.0->pytorch-widedeep) (10.4.0)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-7.0.5-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/61.4 KB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.20.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sentence-transformers->pytorch-widedeep) (0.25.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers->pytorch-widedeep) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers->pytorch-widedeep) (0.20.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers->pytorch-widedeep) (0.4.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/mihirbhupathiraju/Library/Python/3.10/lib/python/site-packages (from transformers->pytorch-widedeep) (6.0.1)\n",
      "Requirement already satisfied: requests in /Users/mihirbhupathiraju/Library/Python/3.10/lib/python/site-packages (from transformers->pytorch-widedeep) (2.31.0)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.0\n",
      "  Downloading thinc-8.3.2-cp310-cp310-macosx_11_0_arm64.whl (779 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.4/779.4 KB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting langcodes<4.0.0,>=3.2.0\n",
      "  Using cached langcodes-3.4.1-py3-none-any.whl (182 kB)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy->pytorch-widedeep) (58.1.0)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.9-cp310-cp310-macosx_11_0_arm64.whl (127 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.8/127.8 KB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m-:--:--\u001b[0m\n",
      "\u001b[?25hCollecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.8-cp310-cp310-macosx_11_0_arm64.whl (491 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 KB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting weasel<0.5.0,>=0.1.0\n",
      "  Using cached weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy->pytorch-widedeep) (2.9.2)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.10-cp310-cp310-macosx_11_0_arm64.whl (26 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.8-cp310-cp310-macosx_11_0_arm64.whl (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 KB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0\n",
      "  Using cached typer-0.12.5-py3-none-any.whl (47 kB)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy->pytorch-widedeep) (2.0.10)\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Using cached wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Collecting lightning-utilities>=0.8.0\n",
      "  Using cached lightning_utilities-0.11.7-py3-none-any.whl (26 kB)\n",
      "Collecting language-data>=1.2\n",
      "  Using cached language_data-1.2.0-py3-none-any.whl (5.4 MB)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->pytorch-widedeep) (2.23.4)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->pytorch-widedeep) (0.7.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/mihirbhupathiraju/Library/Python/3.10/lib/python/site-packages (from python-dateutil>=2.8.2->pandas>=1.3.5->pytorch-widedeep) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mihirbhupathiraju/Library/Python/3.10/lib/python/site-packages (from requests->transformers->pytorch-widedeep) (3.2.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mihirbhupathiraju/Library/Python/3.10/lib/python/site-packages (from requests->transformers->pytorch-widedeep) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mihirbhupathiraju/Library/Python/3.10/lib/python/site-packages (from requests->transformers->pytorch-widedeep) (2023.7.22)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mihirbhupathiraju/Library/Python/3.10/lib/python/site-packages (from requests->transformers->pytorch-widedeep) (3.4)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Using cached confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.0\n",
      "  Downloading thinc-8.3.1.tar.gz (193 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 KB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading thinc-8.3.0.tar.gz (193 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 KB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of srsly to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.7-cp310-cp310-macosx_11_0_arm64.whl (491 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.1/491.1 KB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of spacy-loggers to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Using cached spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
      "INFO: pip is looking at multiple versions of spacy-legacy to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Using cached spacy_legacy-3.0.11-py2.py3-none-any.whl (24 kB)\n",
      "INFO: pip is looking at multiple versions of smart-open to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-7.0.4-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 KB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of safetensors to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting safetensors>=0.4.1\n",
      "  Using cached safetensors-0.4.5-cp310-cp310-macosx_11_0_arm64.whl (381 kB)\n",
      "INFO: pip is looking at multiple versions of requests to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting requests\n",
      "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 KB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of regex to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting regex!=2019.12.17\n",
      "  Using cached regex-2024.9.11-cp310-cp310-macosx_11_0_arm64.whl (284 kB)\n",
      "INFO: pip is looking at multiple versions of pyyaml to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0.2-cp310-cp310-macosx_11_0_arm64.whl (171 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.8/171.8 KB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of pytz to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "INFO: pip is looking at multiple versions of python-dateutil to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting python-dateutil>=2.8.2\n",
      "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 KB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of pydantic-core to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pydantic-core==2.23.4\n",
      "  Using cached pydantic_core-2.23.4-cp310-cp310-macosx_11_0_arm64.whl (1.8 MB)\n",
      "INFO: pip is looking at multiple versions of pydantic to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4\n",
      "  Using cached pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "INFO: pip is looking at multiple versions of preshed to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.8-cp310-cp310-macosx_11_0_arm64.whl (101 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.5/101.5 KB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of pillow to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pillow!=8.3.*,>=5.3.0\n",
      "  Using cached pillow-10.4.0-cp310-cp310-macosx_11_0_arm64.whl (3.4 MB)\n",
      "INFO: pip is looking at multiple versions of packaging to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting packaging\n",
      "  Using cached packaging-24.1-py3-none-any.whl (53 kB)\n",
      "INFO: pip is looking at multiple versions of murmurhash to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.9-cp310-cp310-macosx_11_0_arm64.whl (19 kB)\n",
      "INFO: pip is looking at multiple versions of lightning-utilities to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting lightning-utilities>=0.8.0\n",
      "  Downloading lightning_utilities-0.11.6-py3-none-any.whl (26 kB)\n",
      "INFO: pip is looking at multiple versions of langcodes to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.4.0-py3-none-any.whl (182 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.0/182.0 KB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of joblib to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting joblib>=1.2.0\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "INFO: pip is looking at multiple versions of fsspec to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting fsspec\n",
      "  Using cached fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "INFO: pip is looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting huggingface-hub>=0.20.0\n",
      "  Using cached huggingface_hub-0.25.2-py3-none-any.whl (436 kB)\n",
      "INFO: pip is looking at multiple versions of cymem to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.7-cp310-cp310-macosx_11_0_arm64.whl (30 kB)\n",
      "INFO: pip is looking at multiple versions of cramjam to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting cramjam>=2.3\n",
      "  Downloading cramjam-2.8.3-cp310-cp310-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl (3.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of catalogue to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "INFO: pip is looking at multiple versions of wrapt to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting wrapt\n",
      "  Downloading wrapt-1.15.0-cp310-cp310-macosx_11_0_arm64.whl (36 kB)\n",
      "INFO: pip is looking at multiple versions of torchmetrics to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchmetrics\n",
      "  Downloading torchmetrics-1.4.2-py3-none-any.whl (869 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m869.2/869.2 KB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading torchmetrics-1.4.1-py3-none-any.whl (866 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m866.2/866.2 KB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading torchmetrics-1.4.0.post0-py3-none-any.whl (868 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m868.8/868.8 KB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading torchmetrics-1.4.0-py3-none-any.whl (868 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m868.8/868.8 KB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pretty-errors==1.2.25\n",
      "  Downloading pretty_errors-1.2.25-py3-none-any.whl (17 kB)\n",
      "Collecting colorama\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "INFO: pip is looking at multiple versions of pretty-errors to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchmetrics\n",
      "  Downloading torchmetrics-1.3.2-py3-none-any.whl (841 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m841.5/841.5 KB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading torchmetrics-1.3.1-py3-none-any.whl (840 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.4/840.4 KB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading torchmetrics-1.3.0.post0-py3-none-any.whl (840 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.2/840.2 KB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of torchmetrics to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached torchmetrics-1.2.1-py3-none-any.whl (806 kB)\n",
      "  Downloading torchmetrics-1.2.0-py3-none-any.whl (805 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m805.2/805.2 KB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading torchmetrics-1.1.2-py3-none-any.whl (764 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m764.8/764.8 KB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading torchmetrics-1.1.1-py3-none-any.whl (763 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m763.4/763.4 KB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading torchmetrics-1.1.0-py3-none-any.whl (761 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m761.3/761.3 KB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading torchmetrics-1.0.3-py3-none-any.whl (731 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.6/731.6 KB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading torchmetrics-1.0.2-py3-none-any.whl (731 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.1/731.1 KB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading torchmetrics-1.0.1-py3-none-any.whl (729 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m729.2/729.2 KB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading torchmetrics-1.0.0-py3-none-any.whl (728 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m728.8/728.8 KB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 KB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading torchmetrics-0.11.3-py3-none-any.whl (518 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m518.6/518.6 KB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading torchmetrics-0.11.2-py3-none-any.whl (518 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m518.5/518.5 KB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading torchmetrics-0.11.1-py3-none-any.whl (517 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.2/517.2 KB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading torchmetrics-0.11.0-py3-none-any.whl (512 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.4/512.4 KB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading torchmetrics-0.10.3-py3-none-any.whl (529 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m529.7/529.7 KB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading torchmetrics-0.10.2-py3-none-any.whl (529 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m529.7/529.7 KB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading torchmetrics-0.10.1-py3-none-any.whl (529 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m529.6/529.6 KB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading torchmetrics-0.10.0-py3-none-any.whl (529 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m529.2/529.2 KB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading torchmetrics-0.9.3-py3-none-any.whl (419 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m419.6/419.6 KB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading torchmetrics-0.9.2-py3-none-any.whl (419 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m419.7/419.7 KB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading torchmetrics-0.9.1-py3-none-any.whl (419 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m419.7/419.7 KB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m\n",
      "\u001b[?25h  Downloading torchmetrics-0.9.0-py3-none-any.whl (418 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m418.2/418.2 KB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading torchmetrics-0.8.2-py3-none-any.whl (409 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.8/409.8 KB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyDeprecate==0.3.*\n",
      "  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n",
      "INFO: pip is looking at multiple versions of pydeprecate to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n",
      "  Downloading pyDeprecate-0.3.0-py3-none-any.whl (10 kB)\n",
      "Collecting torchmetrics\n",
      "  Downloading torchmetrics-0.8.1-py3-none-any.whl (408 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m408.9/408.9 KB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading torchmetrics-0.8.0-py3-none-any.whl (408 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m408.6/408.6 KB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of pydeprecate to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading torchmetrics-0.7.3-py3-none-any.whl (398 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m398.2/398.2 KB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading torchmetrics-0.7.2-py3-none-any.whl (397 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m397.2/397.2 KB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading torchmetrics-0.7.1-py3-none-any.whl (397 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m397.2/397.2 KB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m-:--:--\u001b[0m\n",
      "\u001b[?25h  Downloading torchmetrics-0.7.0-py3-none-any.whl (396 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.6/396.6 KB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading torchmetrics-0.6.2-py3-none-any.whl (332 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m332.6/332.6 KB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading torchmetrics-0.6.1-py3-none-any.whl (332 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m332.5/332.5 KB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading torchmetrics-0.6.0-py3-none-any.whl (329 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m329.4/329.4 KB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading torchmetrics-0.5.1-py3-none-any.whl (282 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.0/283.0 KB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading torchmetrics-0.5.0-py3-none-any.whl (272 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m272.0/272.0 KB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading torchmetrics-0.4.1-py3-none-any.whl (234 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m234.8/234.8 KB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading torchmetrics-0.3.2-py3-none-any.whl (274 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.2/274.2 KB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading torchmetrics-0.3.1-py3-none-any.whl (271 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m271.3/271.3 KB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of spacy to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.7.5-cp310-cp310-macosx_11_0_arm64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hCollecting thinc<8.3.0,>=8.2.2\n",
      "  Downloading thinc-8.2.5-cp310-cp310-macosx_11_0_arm64.whl (779 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.0/779.0 KB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.11-cp310-cp310-macosx_11_0_arm64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting shellingham>=1.3.0\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Collecting rich>=10.11.0\n",
      "  Downloading rich-13.9.2-py3-none-any.whl (242 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.1/242.1 KB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting click>=8.0.0\n",
      "  Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 KB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cloudpathlib<1.0.0,>=0.7.0\n",
      "  Downloading cloudpathlib-0.19.0-py3-none-any.whl (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 KB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /Users/mihirbhupathiraju/Library/Python/3.10/lib/python/site-packages (from jinja2->torch>=2.0.0->pytorch-widedeep) (2.1.3)\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 KB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting marisa-trie>=0.7.7\n",
      "  Downloading marisa_trie-1.2.0-cp310-cp310-macosx_11_0_arm64.whl (174 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.1/174.1 KB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting markdown-it-py>=2.2.0\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/mihirbhupathiraju/Library/Python/3.10/lib/python/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->pytorch-widedeep) (2.16.1)\n",
      "Collecting mdurl~=0.1\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Using legacy 'setup.py install' for imutils, since package 'wheel' is not installed.\n",
      "Installing collected packages: sentencepiece, mpmath, imutils, cymem, wrapt, wasabi, typing-extensions, sympy, srsly, spacy-loggers, spacy-legacy, shellingham, scipy, pyarrow, opencv-contrib-python, murmurhash, mdurl, marisa-trie, einops, cramjam, click, blis, torch, smart-open, preshed, markdown-it-py, lightning-utilities, language-data, cloudpathlib, torchvision, torchmetrics, rich, langcodes, gensim, fastparquet, typer, confection, weasel, thinc, sentence-transformers, spacy, pytorch-widedeep\n",
      "  Running setup.py install for imutils ... \u001b[?25ldone\n",
      "\u001b[?25h  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.7.1\n",
      "    Uninstalling typing_extensions-4.7.1:\n",
      "      Successfully uninstalled typing_extensions-4.7.1\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.14.1\n",
      "    Uninstalling scipy-1.14.1:\n",
      "      Successfully uninstalled scipy-1.14.1\n",
      "Successfully installed blis-0.7.11 click-8.1.7 cloudpathlib-0.19.0 confection-0.1.5 cramjam-2.8.4 cymem-2.0.8 einops-0.8.0 fastparquet-2024.5.0 gensim-4.3.3 imutils-0.5.4 langcodes-3.4.1 language-data-1.2.0 lightning-utilities-0.11.7 marisa-trie-1.2.0 markdown-it-py-3.0.0 mdurl-0.1.2 mpmath-1.3.0 murmurhash-1.0.10 opencv-contrib-python-4.10.0.84 preshed-3.0.9 pyarrow-17.0.0 pytorch-widedeep-1.6.3 rich-13.9.2 scipy-1.12.0 sentence-transformers-3.2.0 sentencepiece-0.2.0 shellingham-1.5.4 smart-open-7.0.5 spacy-3.7.5 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 sympy-1.13.3 thinc-8.2.5 torch-2.4.1 torchmetrics-1.4.3 torchvision-0.19.1 typer-0.12.5 typing-extensions-4.12.2 wasabi-1.1.3 weasel-0.4.1 wrapt-1.16.0\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 24.2 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pytorch-widedeep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "lq0elU0J53Yo"
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "import os\n",
    "\n",
    "import random\n",
    "random.seed(SEED)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pytorch_widedeep.preprocessing import TabPreprocessor\n",
    "from pytorch_widedeep.models import TabMlp, WideDeep\n",
    "from pytorch_widedeep import Trainer\n",
    "from pytorch_widedeep.metrics import R2Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aU3xdVpwzuLx"
   },
   "source": [
    "1.Divide the dataset (‘hdb_price_prediction.csv’) into train and test sets by using entries from the year 2020 and before as training data, and entries from 2021 and after as the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_oYG6lNIh7Mp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data: (87370, 11)\n",
      "Testing Data: (72183, 11)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('hdb_price_prediction.csv')\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# Training Data\n",
    "df_train = df[df['year'] <= 2020].copy()\n",
    "# Testing Data\n",
    "df_test = df[df['year'] >= 2021].copy()\n",
    "\n",
    "# Dropping Unncessary Columns\n",
    "df_train.drop(columns=['year','full_address','nearest_stn'], inplace=True)\n",
    "df_test.drop(columns=['year','full_address','nearest_stn'], inplace=True)\n",
    "\n",
    "print(\"Training Data:\", df_train.shape)\n",
    "print(\"Testing Data:\", df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>town</th>\n",
       "      <th>full_address</th>\n",
       "      <th>nearest_stn</th>\n",
       "      <th>dist_to_nearest_stn</th>\n",
       "      <th>dist_to_dhoby</th>\n",
       "      <th>degree_centrality</th>\n",
       "      <th>eigenvector_centrality</th>\n",
       "      <th>flat_model_type</th>\n",
       "      <th>remaining_lease_years</th>\n",
       "      <th>floor_area_sqm</th>\n",
       "      <th>storey_range</th>\n",
       "      <th>resale_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>ANG MO KIO</td>\n",
       "      <td>406 ANG MO KIO AVENUE 10</td>\n",
       "      <td>Ang Mo Kio</td>\n",
       "      <td>1.007264</td>\n",
       "      <td>7.006044</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.006243</td>\n",
       "      <td>2 ROOM, Improved</td>\n",
       "      <td>61.333333</td>\n",
       "      <td>44.0</td>\n",
       "      <td>10 TO 12</td>\n",
       "      <td>232000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>ANG MO KIO</td>\n",
       "      <td>108 ANG MO KIO AVENUE 4</td>\n",
       "      <td>Ang Mo Kio</td>\n",
       "      <td>1.271389</td>\n",
       "      <td>7.983837</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.006243</td>\n",
       "      <td>3 ROOM, New Generation</td>\n",
       "      <td>60.583333</td>\n",
       "      <td>67.0</td>\n",
       "      <td>01 TO 03</td>\n",
       "      <td>250000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>ANG MO KIO</td>\n",
       "      <td>602 ANG MO KIO AVENUE 5</td>\n",
       "      <td>Yio Chu Kang</td>\n",
       "      <td>1.069743</td>\n",
       "      <td>9.090700</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.002459</td>\n",
       "      <td>3 ROOM, New Generation</td>\n",
       "      <td>62.416667</td>\n",
       "      <td>67.0</td>\n",
       "      <td>01 TO 03</td>\n",
       "      <td>262000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>ANG MO KIO</td>\n",
       "      <td>465 ANG MO KIO AVENUE 10</td>\n",
       "      <td>Ang Mo Kio</td>\n",
       "      <td>0.946890</td>\n",
       "      <td>7.519889</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.006243</td>\n",
       "      <td>3 ROOM, New Generation</td>\n",
       "      <td>62.083333</td>\n",
       "      <td>68.0</td>\n",
       "      <td>04 TO 06</td>\n",
       "      <td>265000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>ANG MO KIO</td>\n",
       "      <td>601 ANG MO KIO AVENUE 5</td>\n",
       "      <td>Yio Chu Kang</td>\n",
       "      <td>1.092551</td>\n",
       "      <td>9.130489</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.002459</td>\n",
       "      <td>3 ROOM, New Generation</td>\n",
       "      <td>62.416667</td>\n",
       "      <td>67.0</td>\n",
       "      <td>01 TO 03</td>\n",
       "      <td>265000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   month  year        town              full_address   nearest_stn  \\\n",
       "0      1  2017  ANG MO KIO  406 ANG MO KIO AVENUE 10    Ang Mo Kio   \n",
       "1      1  2017  ANG MO KIO   108 ANG MO KIO AVENUE 4    Ang Mo Kio   \n",
       "2      1  2017  ANG MO KIO   602 ANG MO KIO AVENUE 5  Yio Chu Kang   \n",
       "3      1  2017  ANG MO KIO  465 ANG MO KIO AVENUE 10    Ang Mo Kio   \n",
       "4      1  2017  ANG MO KIO   601 ANG MO KIO AVENUE 5  Yio Chu Kang   \n",
       "\n",
       "   dist_to_nearest_stn  dist_to_dhoby  degree_centrality  \\\n",
       "0             1.007264       7.006044           0.016807   \n",
       "1             1.271389       7.983837           0.016807   \n",
       "2             1.069743       9.090700           0.016807   \n",
       "3             0.946890       7.519889           0.016807   \n",
       "4             1.092551       9.130489           0.016807   \n",
       "\n",
       "   eigenvector_centrality         flat_model_type  remaining_lease_years  \\\n",
       "0                0.006243        2 ROOM, Improved              61.333333   \n",
       "1                0.006243  3 ROOM, New Generation              60.583333   \n",
       "2                0.002459  3 ROOM, New Generation              62.416667   \n",
       "3                0.006243  3 ROOM, New Generation              62.083333   \n",
       "4                0.002459  3 ROOM, New Generation              62.416667   \n",
       "\n",
       "   floor_area_sqm storey_range  resale_price  \n",
       "0            44.0     10 TO 12      232000.0  \n",
       "1            67.0     01 TO 03      250000.0  \n",
       "2            67.0     01 TO 03      262000.0  \n",
       "3            68.0     04 TO 06      265000.0  \n",
       "4            67.0     01 TO 03      265000.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159553 entries, 0 to 159552\n",
      "Data columns (total 14 columns):\n",
      " #   Column                  Non-Null Count   Dtype  \n",
      "---  ------                  --------------   -----  \n",
      " 0   month                   159553 non-null  int64  \n",
      " 1   year                    159553 non-null  int64  \n",
      " 2   town                    159553 non-null  object \n",
      " 3   full_address            159553 non-null  object \n",
      " 4   nearest_stn             159553 non-null  object \n",
      " 5   dist_to_nearest_stn     159553 non-null  float64\n",
      " 6   dist_to_dhoby           159553 non-null  float64\n",
      " 7   degree_centrality       159553 non-null  float64\n",
      " 8   eigenvector_centrality  159553 non-null  float64\n",
      " 9   flat_model_type         159553 non-null  object \n",
      " 10  remaining_lease_years   159553 non-null  float64\n",
      " 11  floor_area_sqm          159553 non-null  float64\n",
      " 12  storey_range            159553 non-null  object \n",
      " 13  resale_price            159553 non-null  float64\n",
      "dtypes: float64(7), int64(2), object(5)\n",
      "memory usage: 17.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_q9PoR50JAA"
   },
   "source": [
    "2.Refer to the documentation of Pytorch-WideDeep and perform the following tasks:\n",
    "https://pytorch-widedeep.readthedocs.io/en/latest/index.html\n",
    "* Use [**TabPreprocessor**](https://pytorch-widedeep.readthedocs.io/en/latest/examples/01_preprocessors_and_utils.html#2-tabpreprocessor) to create the deeptabular component using the continuous\n",
    "features and the categorical features. Use this component to transform the training dataset.\n",
    "* Create the [**TabMlp**](https://pytorch-widedeep.readthedocs.io/en/latest/pytorch-widedeep/model_components.html#pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlp) model with 2 linear layers in the MLP, with 200 and 100 neurons respectively.\n",
    "* Create a [**Trainer**](https://pytorch-widedeep.readthedocs.io/en/latest/pytorch-widedeep/trainer.html#pytorch_widedeep.training.Trainer) for the training of the created TabMlp model with the root mean squared error (RMSE) cost function. Train the model for 100 epochs using this trainer, keeping a batch size of 64. (Note: set the *num_workers* parameter to 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ZBY1iqUXtYWn"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE & RESULT HERE\n",
    "cat_embed_cols = [\n",
    "    ('month', len(np.unique(df['month']))),\n",
    "    ('town', len(np.unique(df['town']))),\n",
    "    ('flat_model_type', len(np.unique(df['flat_model_type']))),\n",
    "    ('storey_range', len(np.unique(df['storey_range']))),\n",
    "]\n",
    "\n",
    "\n",
    "continuous_cols = ['dist_to_nearest_stn','dist_to_dhoby','degree_centrality','eigenvector_centrality',\n",
    "                 'remaining_lease_years','floor_area_sqm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pytorch_widedeep/preprocessing/tab_preprocessor.py:360: UserWarning: Continuous columns will not be normalised\n",
      "  warnings.warn(\"Continuous columns will not be normalised\")\n"
     ]
    }
   ],
   "source": [
    "tab_preprocessor = TabPreprocessor(\n",
    "    cat_embed_cols = cat_embed_cols, continuous_cols = continuous_cols\n",
    ")\n",
    "\n",
    "# Scaled Training Data\n",
    "X_tab = tab_preprocessor.fit_transform(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the TabMlp Model\n",
    "model = TabMlp(tab_preprocessor.column_idx, \n",
    "                cat_embed_input = tab_preprocessor.cat_embed_input, \n",
    "                cat_embed_dropout = 0.1,\n",
    "                continuous_cols = continuous_cols,\n",
    "                mlp_hidden_dims = [200, 100])\n",
    "\n",
    "wide_deep = WideDeep(deeptabular = model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Trainer \n",
    "trainer = Trainer(model = wide_deep,\n",
    "                  objective = \"regression\",\n",
    "                  lr_scheduler_step = False,  \n",
    "                  num_workers = 0,  \n",
    "                  metrics = [R2Score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1: 100%|██████████| 1366/1366 [00:08<00:00, 163.13it/s, loss=5.41e+10, metrics={'r2': -1.2788}]\n",
      "epoch 2: 100%|██████████| 1366/1366 [00:07<00:00, 173.59it/s, loss=1.1e+10, metrics={'r2': 0.535}]  \n",
      "epoch 3: 100%|██████████| 1366/1366 [00:07<00:00, 176.73it/s, loss=5.94e+9, metrics={'r2': 0.7501}]\n",
      "epoch 4: 100%|██████████| 1366/1366 [00:07<00:00, 174.61it/s, loss=4.48e+9, metrics={'r2': 0.8115}]\n",
      "epoch 5: 100%|██████████| 1366/1366 [00:07<00:00, 174.27it/s, loss=4.13e+9, metrics={'r2': 0.8264}]\n",
      "epoch 6: 100%|██████████| 1366/1366 [00:07<00:00, 174.98it/s, loss=3.92e+9, metrics={'r2': 0.8349}]\n",
      "epoch 7: 100%|██████████| 1366/1366 [00:07<00:00, 173.22it/s, loss=3.78e+9, metrics={'r2': 0.8409}]\n",
      "epoch 8: 100%|██████████| 1366/1366 [00:07<00:00, 173.98it/s, loss=3.62e+9, metrics={'r2': 0.8478}]\n",
      "epoch 9: 100%|██████████| 1366/1366 [00:07<00:00, 173.24it/s, loss=3.5e+9, metrics={'r2': 0.8526}] \n",
      "epoch 10: 100%|██████████| 1366/1366 [00:07<00:00, 171.45it/s, loss=3.37e+9, metrics={'r2': 0.858}] \n",
      "epoch 11: 100%|██████████| 1366/1366 [00:08<00:00, 170.19it/s, loss=3.24e+9, metrics={'r2': 0.8636}]\n",
      "epoch 12: 100%|██████████| 1366/1366 [00:08<00:00, 170.17it/s, loss=3.11e+9, metrics={'r2': 0.8692}]\n",
      "epoch 13: 100%|██████████| 1366/1366 [00:07<00:00, 171.99it/s, loss=2.99e+9, metrics={'r2': 0.8741}]\n",
      "epoch 14: 100%|██████████| 1366/1366 [00:08<00:00, 169.91it/s, loss=2.9e+9, metrics={'r2': 0.8779}] \n",
      "epoch 15: 100%|██████████| 1366/1366 [00:08<00:00, 170.64it/s, loss=2.81e+9, metrics={'r2': 0.8817}]\n",
      "epoch 16: 100%|██████████| 1366/1366 [00:08<00:00, 167.72it/s, loss=2.76e+9, metrics={'r2': 0.8839}]\n",
      "epoch 17: 100%|██████████| 1366/1366 [00:08<00:00, 156.90it/s, loss=2.72e+9, metrics={'r2': 0.8856}]\n",
      "epoch 18: 100%|██████████| 1366/1366 [00:08<00:00, 167.85it/s, loss=2.67e+9, metrics={'r2': 0.8877}]\n",
      "epoch 19: 100%|██████████| 1366/1366 [00:08<00:00, 166.67it/s, loss=2.64e+9, metrics={'r2': 0.8892}]\n",
      "epoch 20: 100%|██████████| 1366/1366 [00:08<00:00, 167.21it/s, loss=2.6e+9, metrics={'r2': 0.8907}] \n",
      "epoch 21: 100%|██████████| 1366/1366 [00:08<00:00, 166.47it/s, loss=2.57e+9, metrics={'r2': 0.8919}]\n",
      "epoch 22: 100%|██████████| 1366/1366 [00:08<00:00, 165.39it/s, loss=2.56e+9, metrics={'r2': 0.8923}]\n",
      "epoch 23: 100%|██████████| 1366/1366 [00:08<00:00, 164.54it/s, loss=2.54e+9, metrics={'r2': 0.893}] \n",
      "epoch 24: 100%|██████████| 1366/1366 [00:09<00:00, 146.54it/s, loss=2.52e+9, metrics={'r2': 0.8939}]\n",
      "epoch 25: 100%|██████████| 1366/1366 [00:04<00:00, 327.01it/s, loss=2.49e+9, metrics={'r2': 0.8953}]\n",
      "epoch 26: 100%|██████████| 1366/1366 [00:04<00:00, 339.68it/s, loss=2.49e+9, metrics={'r2': 0.895}] \n",
      "epoch 27: 100%|██████████| 1366/1366 [00:04<00:00, 336.08it/s, loss=2.47e+9, metrics={'r2': 0.8961}]\n",
      "epoch 28: 100%|██████████| 1366/1366 [00:04<00:00, 341.36it/s, loss=2.46e+9, metrics={'r2': 0.8967}]\n",
      "epoch 29: 100%|██████████| 1366/1366 [00:04<00:00, 327.50it/s, loss=2.43e+9, metrics={'r2': 0.8977}]\n",
      "epoch 30: 100%|██████████| 1366/1366 [00:04<00:00, 331.45it/s, loss=2.43e+9, metrics={'r2': 0.8978}]\n",
      "epoch 31: 100%|██████████| 1366/1366 [00:04<00:00, 326.31it/s, loss=2.41e+9, metrics={'r2': 0.8987}]\n",
      "epoch 32: 100%|██████████| 1366/1366 [00:04<00:00, 311.66it/s, loss=2.41e+9, metrics={'r2': 0.8987}]\n",
      "epoch 33: 100%|██████████| 1366/1366 [00:04<00:00, 332.05it/s, loss=2.39e+9, metrics={'r2': 0.8996}]\n",
      "epoch 34: 100%|██████████| 1366/1366 [00:04<00:00, 333.86it/s, loss=2.39e+9, metrics={'r2': 0.8993}]\n",
      "epoch 35: 100%|██████████| 1366/1366 [00:04<00:00, 333.22it/s, loss=2.36e+9, metrics={'r2': 0.9008}]\n",
      "epoch 36: 100%|██████████| 1366/1366 [00:04<00:00, 330.44it/s, loss=2.36e+9, metrics={'r2': 0.9007}]\n",
      "epoch 37: 100%|██████████| 1366/1366 [00:04<00:00, 328.75it/s, loss=2.34e+9, metrics={'r2': 0.9015}]\n",
      "epoch 38: 100%|██████████| 1366/1366 [00:04<00:00, 328.85it/s, loss=2.31e+9, metrics={'r2': 0.9026}]\n",
      "epoch 39: 100%|██████████| 1366/1366 [00:04<00:00, 326.14it/s, loss=2.32e+9, metrics={'r2': 0.9023}]\n",
      "epoch 40: 100%|██████████| 1366/1366 [00:04<00:00, 298.95it/s, loss=2.33e+9, metrics={'r2': 0.902}] \n",
      "epoch 41: 100%|██████████| 1366/1366 [00:04<00:00, 327.20it/s, loss=2.32e+9, metrics={'r2': 0.9022}]\n",
      "epoch 42: 100%|██████████| 1366/1366 [00:04<00:00, 330.35it/s, loss=2.29e+9, metrics={'r2': 0.9036}]\n",
      "epoch 43: 100%|██████████| 1366/1366 [00:04<00:00, 327.42it/s, loss=2.3e+9, metrics={'r2': 0.903}]  \n",
      "epoch 44: 100%|██████████| 1366/1366 [00:04<00:00, 331.95it/s, loss=2.28e+9, metrics={'r2': 0.904}] \n",
      "epoch 45: 100%|██████████| 1366/1366 [00:04<00:00, 327.96it/s, loss=2.29e+9, metrics={'r2': 0.9038}]\n",
      "epoch 46: 100%|██████████| 1366/1366 [00:04<00:00, 328.55it/s, loss=2.27e+9, metrics={'r2': 0.9044}]\n",
      "epoch 47: 100%|██████████| 1366/1366 [00:04<00:00, 325.58it/s, loss=2.26e+9, metrics={'r2': 0.9049}]\n",
      "epoch 48: 100%|██████████| 1366/1366 [00:04<00:00, 329.44it/s, loss=2.25e+9, metrics={'r2': 0.9054}]\n",
      "epoch 49: 100%|██████████| 1366/1366 [00:04<00:00, 329.33it/s, loss=2.24e+9, metrics={'r2': 0.9055}]\n",
      "epoch 50: 100%|██████████| 1366/1366 [00:04<00:00, 327.82it/s, loss=2.25e+9, metrics={'r2': 0.9053}]\n",
      "epoch 51: 100%|██████████| 1366/1366 [00:04<00:00, 327.00it/s, loss=2.25e+9, metrics={'r2': 0.9052}]\n",
      "epoch 52: 100%|██████████| 1366/1366 [00:04<00:00, 320.86it/s, loss=2.24e+9, metrics={'r2': 0.9056}]\n",
      "epoch 53: 100%|██████████| 1366/1366 [00:04<00:00, 331.00it/s, loss=2.22e+9, metrics={'r2': 0.9064}]\n",
      "epoch 54: 100%|██████████| 1366/1366 [00:04<00:00, 326.29it/s, loss=2.23e+9, metrics={'r2': 0.9061}]\n",
      "epoch 55: 100%|██████████| 1366/1366 [00:04<00:00, 325.63it/s, loss=2.22e+9, metrics={'r2': 0.9065}]\n",
      "epoch 56: 100%|██████████| 1366/1366 [00:04<00:00, 324.01it/s, loss=2.21e+9, metrics={'r2': 0.9069}]\n",
      "epoch 57: 100%|██████████| 1366/1366 [00:04<00:00, 311.02it/s, loss=2.22e+9, metrics={'r2': 0.9068}]\n",
      "epoch 58: 100%|██████████| 1366/1366 [00:04<00:00, 325.60it/s, loss=2.21e+9, metrics={'r2': 0.9071}]\n",
      "epoch 59: 100%|██████████| 1366/1366 [00:04<00:00, 325.67it/s, loss=2.21e+9, metrics={'r2': 0.9069}]\n",
      "epoch 60: 100%|██████████| 1366/1366 [00:04<00:00, 325.02it/s, loss=2.2e+9, metrics={'r2': 0.9075}] \n",
      "epoch 61: 100%|██████████| 1366/1366 [00:04<00:00, 325.74it/s, loss=2.19e+9, metrics={'r2': 0.9077}]\n",
      "epoch 62: 100%|██████████| 1366/1366 [00:04<00:00, 324.60it/s, loss=2.18e+9, metrics={'r2': 0.9081}]\n",
      "epoch 63: 100%|██████████| 1366/1366 [00:04<00:00, 325.25it/s, loss=2.17e+9, metrics={'r2': 0.9088}]\n",
      "epoch 64: 100%|██████████| 1366/1366 [00:04<00:00, 324.06it/s, loss=2.2e+9, metrics={'r2': 0.9073}] \n",
      "epoch 65: 100%|██████████| 1366/1366 [00:04<00:00, 322.29it/s, loss=2.17e+9, metrics={'r2': 0.9086}]\n",
      "epoch 66: 100%|██████████| 1366/1366 [00:04<00:00, 308.96it/s, loss=2.16e+9, metrics={'r2': 0.909}] \n",
      "epoch 67: 100%|██████████| 1366/1366 [00:04<00:00, 317.27it/s, loss=2.17e+9, metrics={'r2': 0.9086}]\n",
      "epoch 68: 100%|██████████| 1366/1366 [00:04<00:00, 321.47it/s, loss=2.16e+9, metrics={'r2': 0.9092}]\n",
      "epoch 69: 100%|██████████| 1366/1366 [00:04<00:00, 323.93it/s, loss=2.16e+9, metrics={'r2': 0.9092}]\n",
      "epoch 70: 100%|██████████| 1366/1366 [00:04<00:00, 326.72it/s, loss=2.16e+9, metrics={'r2': 0.9093}]\n",
      "epoch 71: 100%|██████████| 1366/1366 [00:04<00:00, 325.56it/s, loss=2.17e+9, metrics={'r2': 0.9088}]\n",
      "epoch 72: 100%|██████████| 1366/1366 [00:04<00:00, 326.14it/s, loss=2.13e+9, metrics={'r2': 0.9102}]\n",
      "epoch 73: 100%|██████████| 1366/1366 [00:04<00:00, 326.27it/s, loss=2.15e+9, metrics={'r2': 0.9095}]\n",
      "epoch 74: 100%|██████████| 1366/1366 [00:04<00:00, 323.50it/s, loss=2.15e+9, metrics={'r2': 0.9096}]\n",
      "epoch 75: 100%|██████████| 1366/1366 [00:04<00:00, 323.84it/s, loss=2.14e+9, metrics={'r2': 0.9102}]\n",
      "epoch 76: 100%|██████████| 1366/1366 [00:04<00:00, 324.20it/s, loss=2.13e+9, metrics={'r2': 0.9102}]\n",
      "epoch 77: 100%|██████████| 1366/1366 [00:04<00:00, 325.43it/s, loss=2.14e+9, metrics={'r2': 0.9102}]\n",
      "epoch 78: 100%|██████████| 1366/1366 [00:04<00:00, 323.44it/s, loss=2.14e+9, metrics={'r2': 0.9101}]\n",
      "epoch 79: 100%|██████████| 1366/1366 [00:04<00:00, 323.48it/s, loss=2.12e+9, metrics={'r2': 0.9109}]\n",
      "epoch 80: 100%|██████████| 1366/1366 [00:04<00:00, 323.43it/s, loss=2.1e+9, metrics={'r2': 0.9115}] \n",
      "epoch 81: 100%|██████████| 1366/1366 [00:04<00:00, 324.14it/s, loss=2.13e+9, metrics={'r2': 0.9104}]\n",
      "epoch 82: 100%|██████████| 1366/1366 [00:04<00:00, 310.00it/s, loss=2.11e+9, metrics={'r2': 0.9114}]\n",
      "epoch 83: 100%|██████████| 1366/1366 [00:04<00:00, 320.18it/s, loss=2.1e+9, metrics={'r2': 0.9118}] \n",
      "epoch 84: 100%|██████████| 1366/1366 [00:04<00:00, 322.23it/s, loss=2.09e+9, metrics={'r2': 0.912}] \n",
      "epoch 85: 100%|██████████| 1366/1366 [00:04<00:00, 320.72it/s, loss=2.08e+9, metrics={'r2': 0.9123}]\n",
      "epoch 86: 100%|██████████| 1366/1366 [00:04<00:00, 321.04it/s, loss=2.09e+9, metrics={'r2': 0.912}] \n",
      "epoch 87: 100%|██████████| 1366/1366 [00:04<00:00, 315.24it/s, loss=2.1e+9, metrics={'r2': 0.9117}] \n",
      "epoch 88: 100%|██████████| 1366/1366 [00:04<00:00, 311.37it/s, loss=2.08e+9, metrics={'r2': 0.9125}]\n",
      "epoch 89: 100%|██████████| 1366/1366 [00:04<00:00, 319.59it/s, loss=2.07e+9, metrics={'r2': 0.9127}]\n",
      "epoch 90: 100%|██████████| 1366/1366 [00:04<00:00, 314.78it/s, loss=2.08e+9, metrics={'r2': 0.9126}]\n",
      "epoch 91: 100%|██████████| 1366/1366 [00:04<00:00, 323.16it/s, loss=2.07e+9, metrics={'r2': 0.9128}]\n",
      "epoch 92: 100%|██████████| 1366/1366 [00:04<00:00, 320.17it/s, loss=2.07e+9, metrics={'r2': 0.913}] \n",
      "epoch 93: 100%|██████████| 1366/1366 [00:04<00:00, 322.39it/s, loss=2.08e+9, metrics={'r2': 0.9125}]\n",
      "epoch 94: 100%|██████████| 1366/1366 [00:04<00:00, 319.25it/s, loss=2.08e+9, metrics={'r2': 0.9123}]\n",
      "epoch 95: 100%|██████████| 1366/1366 [00:04<00:00, 315.70it/s, loss=2.06e+9, metrics={'r2': 0.9133}]\n",
      "epoch 96: 100%|██████████| 1366/1366 [00:04<00:00, 310.18it/s, loss=2.06e+9, metrics={'r2': 0.9133}]\n",
      "epoch 97: 100%|██████████| 1366/1366 [00:04<00:00, 315.12it/s, loss=2.05e+9, metrics={'r2': 0.9138}]\n",
      "epoch 98: 100%|██████████| 1366/1366 [00:04<00:00, 313.89it/s, loss=2.05e+9, metrics={'r2': 0.9137}]\n",
      "epoch 99: 100%|██████████| 1366/1366 [00:04<00:00, 317.58it/s, loss=2.05e+9, metrics={'r2': 0.9139}]\n",
      "epoch 100: 100%|██████████| 1366/1366 [00:04<00:00, 312.93it/s, loss=2.04e+9, metrics={'r2': 0.914}] \n"
     ]
    }
   ],
   "source": [
    "# Training Model with 100 Epochs with Batch Size 64\n",
    "trainer.fit(X_tab = X_tab, \n",
    "            target = df_train['resale_price'].values, \n",
    "            n_epochs = 100, \n",
    "            batch_size = 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V46s-MdM0y5c"
   },
   "source": [
    "3.Report the test RMSE and the test R2 value that you obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "KAhAgvMC07g6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict: 100%|██████████| 1128/1128 [00:01<00:00, 915.95it/s]\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE & RESULT HERE\n",
    "X_test = tab_preprocessor.transform(df_test)\n",
    "\n",
    "# Make predictions on the test dataset\n",
    "y_pred = trainer.predict(X_tab = X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([201287.66, 211353.58, 288670.  , ..., 590245.3 , 520264.7 ,\n",
       "       540961.8 ], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 Score: 0.6359610250405704\n",
      "Root Mean Squared Error (RMSE): 102076.24282375441\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_true = df_test['resale_price']\n",
    "\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "print(\"R^2 Score:\", r2)\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "print(\"Root Mean Squared Error (RMSE):\", np.sqrt(mse))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
